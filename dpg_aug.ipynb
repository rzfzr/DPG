{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    classification_report\n",
    ")\n",
    "import math\n",
    "\n",
    "from scipy.spatial.distance import euclidean, cityblock, cosine\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "PATH_EVU = \"experiment/data/evolution\"\n",
    "os.makedirs(PATH_EVU, exist_ok=True)\n",
    "\n",
    "class TwoClassTargetDPG_GA:\n",
    "    def __init__(self,\n",
    "                 population_size,\n",
    "                 mutation_rate,\n",
    "                 crossover_rate,\n",
    "                 model,\n",
    "                 target_class,\n",
    "                 border_class,\n",
    "                 n_classes,\n",
    "                 class_bounds,\n",
    "                 augmentation_mode,  # \"traditional\" or \"border\"\n",
    "                 boundary_points,\n",
    "                 boundary_weight,\n",
    "                 max_other_prob,\n",
    "                 diversity_weight,\n",
    "                 re_inject_threshold,  # not used\n",
    "                 re_inject_ratio,        # not used\n",
    "                 repulsion_weight,\n",
    "                 feature_order,\n",
    "                 random_seed,\n",
    "                 default_intervals,\n",
    "                 sample,\n",
    "                 distance_factor,\n",
    "                 sparsity_factor,\n",
    "                 constraints_factor):\n",
    "        \n",
    "        random.seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        self.population_size = population_size\n",
    "        self.mutation_rate = mutation_rate\n",
    "        self.crossover_rate = crossover_rate\n",
    "        self.model = model\n",
    "        # self.target_class = target_class % n_classes\n",
    "        # self.border_class = border_class % n_classes\n",
    "        self.target_class = target_class\n",
    "        self.border_class = border_class\n",
    "        self.n_classes = n_classes\n",
    "        self.class_bounds = class_bounds\n",
    "        self.augmentation_mode = augmentation_mode\n",
    "        self.boundary_points = boundary_points  # not used in \"traditional\" mode\n",
    "        self.boundary_weight = boundary_weight\n",
    "        self.max_other_prob = max_other_prob\n",
    "        self.diversity_weight = diversity_weight\n",
    "        self.re_inject_threshold = re_inject_threshold\n",
    "        self.re_inject_ratio = re_inject_ratio\n",
    "        self.repulsion_weight = repulsion_weight\n",
    "        self.feature_order = feature_order\n",
    "        # Use provided bounds if available; otherwise fallback to default_intervals.\n",
    "        self.target_intervals = self.class_bounds.get(f\"Class {self.target_class}\", default_intervals)\n",
    "        self.sample = sample\n",
    "        self.distance_factor = distance_factor\n",
    "        self.sparsity_factor = sparsity_factor\n",
    "        self.constraints_factor = constraints_factor\n",
    "        \n",
    "    # def random_individual(self):\n",
    "    #     ind = [random.uniform(self.target_intervals[feat][0], self.target_intervals[feat][1])\n",
    "    #            for feat in self.feature_order]\n",
    "    #     return np.array(ind) if self.augmentation_mode == \"traditional\" else ind\n",
    "    def random_individual(self):\n",
    "        original = self.sample.flatten()[:-1]  # Assuming last value is the target\n",
    "        num_features = len(original)\n",
    "        min_changes = int(0.05 * num_features)\n",
    "        max_changes = int(0.1 * num_features)\n",
    "        num_changes = random.randint(min_changes, max_changes)\n",
    "        \n",
    "        # Choose random feature indices to change\n",
    "        change_indices = random.sample(range(num_features), num_changes)\n",
    "        \n",
    "        # Copy original\n",
    "        new_individual = original.copy()\n",
    "        \n",
    "        # Apply random changes to selected features\n",
    "        for i in change_indices:\n",
    "            feat_name = self.feature_order[i]\n",
    "            low, high = self.target_intervals[feat_name]\n",
    "            new_individual[i] = random.uniform(low, high)\n",
    "    \n",
    "        return new_individual if self.augmentation_mode == \"traditional\" else new_individual.tolist()\n",
    "\n",
    "    def initialize_population(self):\n",
    "        if self.augmentation_mode == \"traditional\":\n",
    "            return np.array([self.random_individual() for _ in range(self.population_size)])\n",
    "        else:\n",
    "            return [self.random_individual() for _ in range(self.population_size)]\n",
    "\n",
    "    def mutate(self, pop: np.ndarray) -> np.ndarray:\n",
    "        for i in range(len(pop)):\n",
    "            if random.random() < self.mutation_rate:\n",
    "                for j, feat in enumerate(self.feature_order):\n",
    "                    low, high = self.target_intervals[feat]\n",
    "                    r = (high - low) * 0.05\n",
    "                    delta = random.uniform(-r, r)\n",
    "                    pop[i, j] += delta\n",
    "                    pop[i, j] = max(low, min(pop[i, j], high))\n",
    "        return pop\n",
    "    # check size\n",
    "    def mutate_individual(self, ind: List[float]) -> List[float]:\n",
    "        new_ind = ind.copy()\n",
    "        for j, feat in enumerate(self.feature_order):\n",
    "            low, high = self.target_intervals[feat]\n",
    "            r = (high - low) * 0.05\n",
    "            delta = random.uniform(-r, r)\n",
    "            new_ind[j] += delta\n",
    "            new_ind[j] = max(low, min(new_ind[j], high))\n",
    "        return new_ind\n",
    "\n",
    "    def crossover(self, pop: np.ndarray) -> np.ndarray:\n",
    "        children = []\n",
    "        for i in range(0, len(pop), 2):\n",
    "            p1, p2 = pop[i], pop[i + 1]\n",
    "            if random.random() < self.crossover_rate:\n",
    "                point = random.randint(1, len(p1) - 1)\n",
    "                c1 = np.concatenate([p1[:point], p2[point:]])\n",
    "                c2 = np.concatenate([p2[:point], p1[point:]])\n",
    "                children.extend([c1, c2])\n",
    "            else:\n",
    "                children.extend([p1, p2])\n",
    "        return np.array(children)\n",
    "\n",
    "    def crossover_individuals(self, ind1: List[float], ind2: List[float]) -> Tuple[List[float], List[float]]:\n",
    "        point = random.randint(1, len(ind1) - 1)\n",
    "        c1 = ind1[:point] + ind2[point:]\n",
    "        c2 = ind2[:point] + ind1[point:]\n",
    "        return c1, c2\n",
    "\n",
    "    def fitness_function_traditional(self, pop: np.ndarray) -> np.ndarray:\n",
    "        fitness = np.zeros(len(pop))        \n",
    "        sample_features = self.sample.flatten()[:-1]  # remove target\n",
    "        \n",
    "        for i, individual in enumerate(pop):\n",
    "            X_pred = individual.reshape(1, -1)\n",
    "            predicted_class = self.model.predict(X_pred)\n",
    "            constraints_score = round(self.calculate_constraint_violation(individual),4)\n",
    "            distance_score = round(self.calculate_distance(sample_features, individual), 4)\n",
    "            sparsity_score = round(self.calculate_sparsity(sample_features, individual), 4)\n",
    "            \n",
    "            fitness[i] += (1 - constraints_score) * self.constraints_factor\n",
    "            fitness[i] += distance_score * self.distance_factor\n",
    "            fitness[i] += (1 - sparsity_score) * self.sparsity_factor\n",
    "            \n",
    "            # print(f\"Constraint Violation: {(1 - constraints_score) * self.constraints_factor}\")\n",
    "            # print(f\"Distance: {distance_score * self.distance_factor}\")\n",
    "            # print(f\"Sparsity: {(1 - sparsity_score) * self.sparsity_factor}\")\n",
    "            # print(f\"Predicted Class: {predicted_class} Minority Class: {self.target_class}\")\n",
    "            if predicted_class != self.target_class:\n",
    "                fitness[i] *= -1\n",
    "        return fitness\n",
    "\n",
    "    def base_border_fitness(self, ind: List[float]) -> float:\n",
    "        INVALID_FITNESS = -50.0\n",
    "        for feat in self.feature_order:\n",
    "            low, high = self.target_intervals[feat]\n",
    "            val = ind[self.feature_order.index(feat)]\n",
    "            if not (low <= val <= high):\n",
    "                return INVALID_FITNESS\n",
    "        arr2d = np.array(ind)\n",
    "        probs = self.model.predict_proba([arr2d])[0]\n",
    "        pred = self.model.predict([arr2d])[0]\n",
    "        if pred != self.target_class:\n",
    "            return INVALID_FITNESS\n",
    "        p_t = probs[self.target_class]\n",
    "        p_b = probs[self.border_class]\n",
    "        boundary_score = 1.0 - abs(p_t - p_b)\n",
    "        for c in range(self.n_classes):\n",
    "            if c not in (self.target_class, self.border_class):\n",
    "                if probs[c] > self.max_other_prob:\n",
    "                    return INVALID_FITNESS\n",
    "        return 5.0 * p_t + 3.0 * boundary_score\n",
    "\n",
    "    def individual_diversity(self, ind: List[float], pop: List[List[float]]) -> float:\n",
    "        distances = [np.linalg.norm(np.array(ind) - np.array(other)) for other in pop if other != ind]\n",
    "        return np.mean(distances) if distances else 0.0\n",
    "\n",
    "    def min_distance_to_others(self, ind: List[float], pop: List[List[float]]) -> float:\n",
    "        distances = [np.linalg.norm(np.array(ind) - np.array(other)) for other in pop if other != ind]\n",
    "        return min(distances) if distances else 0.0\n",
    "\n",
    "    def distance_to_boundary_line(self, ind: List[float]) -> float:\n",
    "        return 0.05\n",
    "\n",
    "    def compute_population_diversity(self, pop: List[List[float]]) -> float:\n",
    "        if len(pop) < 2:\n",
    "            return 0.0\n",
    "        distances = [np.linalg.norm(np.array(ind1) - np.array(ind2))\n",
    "                     for i, ind1 in enumerate(pop) for ind2 in pop[i+1:]]\n",
    "        return np.mean(distances)\n",
    "\n",
    "    def total_fitness(self, ind: List[float], pop: List[List[float]]) -> float:\n",
    "        base = self.base_border_fitness(ind)\n",
    "        if base < -40:\n",
    "            return base\n",
    "        div = self.individual_diversity(ind, pop)\n",
    "        div_bonus = self.diversity_weight * div\n",
    "        min_d = self.min_distance_to_others(ind, pop)\n",
    "        rep_bonus = self.repulsion_weight * min_d\n",
    "        dist_line = self.distance_to_boundary_line(ind)\n",
    "        line_bonus = 1.0 / (1.0 + dist_line) * self.boundary_weight\n",
    "        penalty = -50.0 if dist_line > 0.1 else 0.0\n",
    "        return base + div_bonus + rep_bonus + line_bonus + penalty\n",
    "\n",
    "    def get_fitness(self, ind: List[float], pop: List[List[float]]) -> float:\n",
    "        if self.augmentation_mode == \"traditional\":\n",
    "            arr = np.array(ind).reshape(1, -1)\n",
    "            return self.fitness_function_traditional(arr)[0]\n",
    "        else:\n",
    "            return self.total_fitness(ind, pop)\n",
    "\n",
    "    def select_parents_tournament(self, pop: List[List[float]], t_size=3):\n",
    "        def tournament():\n",
    "            candidates = random.sample(pop, t_size)\n",
    "            return max(candidates, key=lambda c: self.get_fitness(c, pop))\n",
    "        return tournament(), tournament()\n",
    "        \n",
    "    def calculate_sparsity(self, original_sample: np.ndarray, individual_sample: np.ndarray) -> float:\n",
    "        # Ensure inputs are 1D arrays\n",
    "        original_sample = original_sample.flatten()\n",
    "        individual_sample = individual_sample.flatten()\n",
    "        \n",
    "        # Count how many features differ\n",
    "        changed_features = np.sum(original_sample != individual_sample)\n",
    "        \n",
    "        # Ratio of changed features\n",
    "        # 1 -> every feature changed\n",
    "        # 0 -> none\n",
    "        sparsity = changed_features / len(original_sample)\n",
    "        return sparsity\n",
    "    \n",
    "    def calculate_distance(self, original_sample: np.ndarray, individual_sample: np.ndarray) -> float:\n",
    "        original_sample = np.asarray(original_sample)\n",
    "        individual_sample = np.asarray(individual_sample)\n",
    "\n",
    "        # Avoid division by zero\n",
    "        denominator = np.where(original_sample == 0, 1e-8, original_sample)\n",
    "        \n",
    "        relative_distance = np.abs((individual_sample - original_sample) / denominator)\n",
    "        return np.mean(relative_distance)\n",
    "    \n",
    "    def calculate_constraint_violation(self, individual: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the percentage of features that are outside the allowed target_intervals.\n",
    "        \n",
    "        Parameters:\n",
    "            individual (np.ndarray): 1D array of feature values for a candidate individual.\n",
    "        \n",
    "        Returns:\n",
    "            float: Ratio (0 to 1) of features that violate their interval constraints.\n",
    "        \"\"\"\n",
    "        violations = 0\n",
    "        total_features = len(self.feature_order)\n",
    "\n",
    "        for i, feat in enumerate(self.feature_order):\n",
    "            low, high = self.target_intervals[feat]\n",
    "            if not (low <= individual[i] <= high):\n",
    "                violations += 1\n",
    "\n",
    "        return violations / total_features\n",
    "\n",
    "    # class -00 1\n",
    "    # features 0 a 1\n",
    "    def evolve(self, generations=20, tournament_size=3, stagnation_limit=5, dataset_name=None, perc=None):\n",
    "        if self.augmentation_mode == \"traditional\":\n",
    "            pop = self.initialize_population()\n",
    "            # print(f\"Population: {len(pop)}\")\n",
    "            # with open(\"population_dump.txt\", \"w\") as f:\n",
    "            #     for individual in pop:\n",
    "            #         f.write(str(individual) + \"\\n\")\n",
    "            best_fit_so_far = float('-inf')\n",
    "            no_improve = 0\n",
    "            log_df = pd.DataFrame(columns=[\"generation\", \"fitness\", \"individual\"])\n",
    "            for gen in range(generations):\n",
    "                # print(f\"Generation: {gen}\")\n",
    "                fitness = self.fitness_function_traditional(pop)\n",
    "                best_ind = pop[np.argmax(fitness)]\n",
    "                best_val = np.max(fitness)\n",
    "                # print(f\"Best Ind: {best_val}\")\n",
    "                # print(f\"Fitness: {fitness}\")\n",
    "\n",
    "                log_df = pd.concat([\n",
    "                    log_df,\n",
    "                    pd.DataFrame([{\n",
    "                        \"generation\": gen,\n",
    "                        \"fitness\": float(best_val),\n",
    "                        \"individual\": best_ind.tolist(),\n",
    "                    }])\n",
    "                ], ignore_index=True)\n",
    "\n",
    "                # print(f\"Best Individual {best_ind}\")\n",
    "                new_pop = [best_ind]\n",
    "\n",
    "                while len(new_pop) < self.population_size:\n",
    "                    # print(f\"NEW POP {new_pop}\")\n",
    "                    p1, p2 = self.select_parents_tournament(list(pop), tournament_size)\n",
    "                    # print(\"Tournament\")\n",
    "                    if random.random() < self.crossover_rate:\n",
    "                        # print(\"Crossover\")\n",
    "                        # print(f\"TYPE: {type(p1)}\")\n",
    "                        c1, c2 = self.crossover_individuals(list(p1), list(p2))\n",
    "                        # print(\"Crossover!\")\n",
    "                    else:\n",
    "                        c1, c2 = p1[:], p2[:]\n",
    "                    if random.random() < self.mutation_rate:\n",
    "                        # print(f\"C1: {len(c1)}\")\n",
    "                        c1 = self.mutate_individual(c1)\n",
    "                        # print(\"Mutation\")\n",
    "                    if random.random() < self.mutation_rate:\n",
    "                        # print(f\"C2: {len(c2)}\")\n",
    "                        c2 = self.mutate_individual(c2)\n",
    "                        # print(\"Mutation2\")\n",
    "                    new_pop.extend([c1, c2])\n",
    "                pop = new_pop[:self.population_size]\n",
    "                pop = np.array(pop)\n",
    "                if best_val > best_fit_so_far:\n",
    "                    best_fit_so_far = best_val\n",
    "                    no_improve = 0\n",
    "                else:\n",
    "                    no_improve += 1\n",
    "                if no_improve >= stagnation_limit:\n",
    "                    print(f\"[Early Stop Gen {gen}] No improvement for {stagnation_limit} generations.\")\n",
    "                    break\n",
    "            # Save the log to CSV or Parquet\n",
    "            log_df.to_csv(f\"{PATH_EVU}/{dataset_name}_{perc}_evolution_log.csv\", index=False)\n",
    "            return pop\n",
    "        else:\n",
    "            pop = self.initialize_population()\n",
    "            best_fit_so_far = float('-inf')\n",
    "            no_improve = 0\n",
    "            for gen in range(generations):\n",
    "                best_ind = max(pop, key=lambda c: self.get_fitness(c, pop))\n",
    "                best_val = self.get_fitness(best_ind, pop)\n",
    "                new_pop = [best_ind]\n",
    "                while len(new_pop) < self.population_size:\n",
    "                    p1, p2 = self.select_parents_tournament(pop, tournament_size)\n",
    "                    if random.random() < self.crossover_rate:\n",
    "                        c1, c2 = self.crossover_individuals(p1, p2)\n",
    "                    else:\n",
    "                        c1, c2 = p1[:], p2[:]\n",
    "                    if random.random() < self.mutation_rate:\n",
    "                        c1 = self.mutate_individual(c1)\n",
    "                    if random.random() < self.mutation_rate:\n",
    "                        c2 = self.mutate_individual(c2)\n",
    "                    new_pop.extend([c1, c2])\n",
    "                pop = new_pop[:self.population_size]\n",
    "                if best_val > best_fit_so_far:\n",
    "                    best_fit_so_far = best_val\n",
    "                    no_improve = 0\n",
    "                else:\n",
    "                    no_improve += 1\n",
    "                if no_improve >= stagnation_limit:\n",
    "                    print(f\"[Early Stop Gen {gen}] No improvement for {stagnation_limit} generations.\")\n",
    "                    break\n",
    "            return pop\n",
    "\n",
    "    def generate_samples(self, num_samples=5, generations=20, dataset_name=None, perc=None) -> np.ndarray:\n",
    "        results = []\n",
    "        for i in range(num_samples):\n",
    "            if self.augmentation_mode == \"traditional\":\n",
    "                final_pop = self.evolve(generations=generations, dataset_name=dataset_name, perc=perc)\n",
    "                trad_fitness = self.fitness_function_traditional(final_pop)\n",
    "                best_ind = final_pop[np.argmax(trad_fitness)]\n",
    "            else:\n",
    "                final_pop = self.evolve(generations=generations, dataset_name=dataset_name, perc=perc)\n",
    "                best_ind = max(final_pop, key=lambda c: self.get_fitness(c, final_pop))\n",
    "            results.append(best_ind)\n",
    "        return np.array(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Violations - Section 6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Violations eval\n",
    "def evaluate_healthcare_constraints(data):\n",
    "    \"\"\"Evaluate constraints for the healthcare dataset.\"\"\"\n",
    "    violations = []\n",
    "    for idx, row in data.iterrows():\n",
    "        if row['Age'] < 0 or row['Age'] > 120:\n",
    "            violations.append((idx, 'Invalid Age'))\n",
    "        if row['BMI'] < 10 or row['BMI'] > 50:\n",
    "            violations.append((idx, 'Invalid BMI'))\n",
    "        if row['Cholesterol'] < 0:\n",
    "            violations.append((idx, 'Invalid Cholesterol'))\n",
    "    return violations\n",
    "\n",
    "def evaluate_finance_constraints(data):\n",
    "    \"\"\"Evaluate constraints for the finance dataset.\"\"\"\n",
    "    violations = []\n",
    "    for idx, row in data.iterrows():\n",
    "        if row['Income'] < 0:\n",
    "            violations.append((idx, 'Negative Income'))\n",
    "        if row['CreditScore'] < 300 or row['CreditScore'] > 850:\n",
    "            violations.append((idx, 'Invalid Credit Score'))\n",
    "        # if row['MaritalStatus'] == 'Single' and row['NumChildren'] > 0:\n",
    "        #     violations.append((idx, 'Single with Children'))\n",
    "    return violations\n",
    "\n",
    "def evaluate_quality_control_constraints(data):\n",
    "    \"\"\"Evaluate constraints for the quality control dataset.\"\"\"\n",
    "    violations = []\n",
    "    for idx, row in data.iterrows():\n",
    "        if row['Temperature'] < 0 or row['Temperature'] > 150:\n",
    "            violations.append((idx, 'Invalid Temperature'))\n",
    "        if row['Pressure'] < 0 or row['Pressure'] > 20:\n",
    "            violations.append((idx, 'Invalid Pressure'))\n",
    "        if row['Speed'] < 0:\n",
    "            violations.append((idx, 'Negative Speed'))\n",
    "        if row['Vibration'] < 0:\n",
    "            violations.append((idx, 'Negative Vibration'))\n",
    "    return violations\n",
    "\n",
    "def evaluate_fraud_detection_constraints(data):\n",
    "    \"\"\"Evaluate constraints for the fraud detection dataset.\"\"\"\n",
    "    violations = []\n",
    "    for idx, row in data.iterrows():\n",
    "        if row['TransactionAmount'] < 0:\n",
    "            violations.append((idx, 'Negative Transaction Amount'))\n",
    "        if row['TransactionTime'] < 0:\n",
    "            violations.append((idx, 'Negative Transaction Time'))\n",
    "    return violations\n",
    "\n",
    "def evaluate_energy_constraints(data):\n",
    "    \"\"\"Evaluate constraints for the energy dataset.\"\"\"\n",
    "    violations = []\n",
    "    for idx, row in data.iterrows():\n",
    "        if row['Usage'] < 0:\n",
    "            violations.append((idx, 'Negative Usage'))\n",
    "        if row['Voltage'] < 190 or row['Voltage'] > 250:\n",
    "            violations.append((idx, 'Invalid Voltage'))\n",
    "    return violations\n",
    "\n",
    "def evaluate_education_constraints(data):\n",
    "    \"\"\"Evaluate constraints for the education dataset.\"\"\"\n",
    "    violations = []\n",
    "    for idx, row in data.iterrows():\n",
    "        if row['Attendance'] < 0 or row['Attendance'] > 100:\n",
    "            violations.append((idx, 'Invalid Attendance'))\n",
    "        if row['StudyHours'] < 0:\n",
    "            violations.append((idx, 'Negative Study Hours'))\n",
    "        if row['Grades'] < 0 or row['Grades'] > 100:\n",
    "            violations.append((idx, 'Invalid Grades'))\n",
    "    return violations\n",
    "\n",
    "constraint_evaluators = {\n",
    "    'healthcare_dataset': evaluate_healthcare_constraints,\n",
    "    'finance_dataset': evaluate_finance_constraints,\n",
    "    'quality_control_dataset': evaluate_quality_control_constraints,\n",
    "    'fraud_detection_dataset': evaluate_fraud_detection_constraints,\n",
    "    'energy_dataset': evaluate_energy_constraints,\n",
    "    'education_dataset': evaluate_education_constraints,\n",
    "}\n",
    "\n",
    "def evaluate_constraints(dataset_name, data):\n",
    "    \"\"\"Evaluate constraints dynamically based on the dataset name.\"\"\"\n",
    "    if dataset_name not in constraint_evaluators:\n",
    "        raise ValueError(f\"Unknown dataset: {dataset_name}\")\n",
    "\n",
    "    evaluator = constraint_evaluators[dataset_name]\n",
    "    return evaluator(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "\n",
    "PATH = \"extract_constraints/data/synthetic_data\"\n",
    "results_path = \"experiment/result/\"\n",
    "os.makedirs(PATH, exist_ok=True)\n",
    "\n",
    "def get_minority_idx(y):\n",
    "    return list(y.unique()).index(y.value_counts().idxmin())\n",
    "\n",
    "classifiers = {\n",
    "        \"LogisticRegression\": LogisticRegression(random_state=42, max_iter=1000),\n",
    "        \"kNN\": KNeighborsClassifier(),\n",
    "        \"DecisionTree\": DecisionTreeClassifier(random_state=42)\n",
    "    }\n",
    "\n",
    "distance_factor=2\n",
    "sparsity_factor=1\n",
    "constraints_factor=3\n",
    "\n",
    "# for dataset in os.listdir(PATH):\n",
    "for dataset in [\"education_dataset\", \"energy_dataset\", \"finance_dataset\", \"fraud_detection_dataset\", \"healthcare_dataset\", \"quality_control_dataset\"]:\n",
    "    print(f\"Dataset {dataset}\")\n",
    "    overall_start = time.time()\n",
    "    \n",
    "    dataset_path = f\"{PATH}/{dataset}\"\n",
    "    \n",
    "    X_test = pd.read_csv(f\"{dataset_path}/X_test.csv\")\n",
    "    y_test = pd.read_csv(f\"{dataset_path}/y_test.csv\")\n",
    "    \n",
    "    y_test = y_test.iloc[:, -1]\n",
    "\n",
    "    full_data = pd.read_csv(f\"{PATH}/{dataset}.csv\")  # Replace with actual file name\n",
    "    \n",
    "    # Split X and y\n",
    "    X_full = full_data.iloc[:, :-1]  # all columns except the last\n",
    "    y_full = full_data.iloc[:, -1]   # the last column is y\n",
    "\n",
    "    # Remove test rows from full dataset to get training set\n",
    "    X_train = X_full[~X_full.apply(tuple, axis=1).isin(X_test.apply(tuple, axis=1))]\n",
    "    numerical_cols = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    y_train = y_full.loc[X_train.index]\n",
    "    \n",
    "    X_train = X_train[numerical_cols]\n",
    "    features = numerical_cols\n",
    "    feature_min_max = {feat: (X_train[feat].min(), X_train[feat].max()) for feat in features}\n",
    "    X_train = X_train.to_numpy()\n",
    "    \n",
    "    model = RandomForestClassifier(n_estimators=3, random_state=42, class_weight='balanced')\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Generate classification report\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    # print(report)\n",
    "    \n",
    "    with open(f\"{dataset_path}/constraints.json\", \"r\") as f:\n",
    "        constraints = json.load(f)  # constraints is now a Python dict\n",
    "    \n",
    "    target_class = f\"Class {get_minority_idx(y_train)}\"\n",
    "    print(f\"Target Class: {target_class}\")\n",
    "    \n",
    "    class_counts = pd.Series(y_train).value_counts()\n",
    "    majority_class = class_counts.idxmax()\n",
    "    minority_class = class_counts.idxmin()\n",
    "    majority_count = class_counts.max()\n",
    "    minority_count = class_counts.min()\n",
    "\n",
    "    X_test_minority = X_test[y_test == minority_class]\n",
    "    X_test_minority_np = X_test_minority.to_numpy()\n",
    "\n",
    "    sample_size = min(1, len(X_test_minority_np)) # just in case we want to pass a group of samples later\n",
    "    X_test_np = X_test.to_numpy()\n",
    "\n",
    "    # Sample indices from minority class only\n",
    "    sample_indices = np.random.default_rng(seed=42).choice(len(X_test_minority_np), size=sample_size, replace=False)\n",
    "    sample_features = X_test_minority_np[sample_indices]\n",
    "\n",
    "    # Create corresponding labels\n",
    "    sample_labels = np.full((sample_size, 1), minority_class)\n",
    "\n",
    "    # Combine features and label\n",
    "    sample = np.hstack([sample_features, sample_labels])\n",
    "    # print(sample)\n",
    "    # break\n",
    "    try:\n",
    "        ga = TwoClassTargetDPG_GA(\n",
    "                        population_size=30,\n",
    "                        mutation_rate=0.3,\n",
    "                        crossover_rate=0.7,\n",
    "                        model=model,\n",
    "                        target_class=minority_class,\n",
    "                        border_class=minority_class,\n",
    "                        n_classes=len(np.unique(y_train)),\n",
    "                        class_bounds=constraints,\n",
    "                        augmentation_mode=\"traditional\",\n",
    "                        boundary_points=None,\n",
    "                        boundary_weight=15.0,\n",
    "                        max_other_prob=0.2,\n",
    "                        diversity_weight=0.5,\n",
    "                        re_inject_threshold=0.5,\n",
    "                        re_inject_ratio=0.2,\n",
    "                        repulsion_weight=4.0,\n",
    "                        feature_order=features,\n",
    "                        random_seed=123,\n",
    "                        default_intervals=feature_min_max,\n",
    "                        sample = sample,\n",
    "                        distance_factor=distance_factor,\n",
    "                        sparsity_factor=sparsity_factor,\n",
    "                        constraints_factor=constraints_factor\n",
    "                        )\n",
    "        \n",
    "        augmentation_percentages = [0.05, 0.15, 0.3, 0.5]\n",
    "        \n",
    "        for perc in augmentation_percentages:\n",
    "            violations_list = []\n",
    "            print(f\"===========\\n Augmentation Percentage {perc}\\n\")\n",
    "            results_list = []\n",
    "            new_count = int(math.ceil(minority_count * (1 + perc)))\n",
    "            gen_start = time.time()\n",
    "            synthetic_data = ga.generate_samples(num_samples=abs(new_count-minority_count), generations=20, dataset_name=dataset, perc=perc)\n",
    "            \n",
    "            gen_time = time.time() - gen_start\n",
    "            X_train_aug = np.vstack((X_train, synthetic_data))\n",
    "            synthetic_labels = np.full((len(synthetic_data),), minority_class)\n",
    "            X_train_aug = pd.DataFrame(X_train_aug, columns=features)\n",
    "\n",
    "            violations = evaluate_constraints(dataset, X_train_aug)            \n",
    "            violations_dict = {\"Dataset\": dataset,\n",
    "                                \"Method\":'DPG-da',\n",
    "                                \"Aug\": perc,\n",
    "                                \"Samples\": new_count,\n",
    "                                \"Number of Violation\": len(violations),\n",
    "                                \"Violations\":violations,\n",
    "                                \"Rep\":1}\n",
    "            violations_list.append(violations_dict)\n",
    "        \n",
    "            file_exists = os.path.isfile(f\"{results_path}/violations_dpg.csv\")\n",
    "            violations_df = pd.DataFrame(violations_list)\n",
    "            violations_df.to_csv(f\"{results_path}/violations_dpg.csv\", index=False, mode=\"a\", header= not file_exists)\n",
    "    except Exception as e:\n",
    "        print(f\"Dataset {dataset} - {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "PATH = \"experiment/data/violation_data\"\n",
    "\n",
    "def read_keel_dat(filepath):\n",
    "    attributes = {}\n",
    "    data_started = False\n",
    "    data_rows = []\n",
    "\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith('%'):\n",
    "                continue  # skip empty/comment lines\n",
    "\n",
    "            if line.lower().startswith('@attribute'):\n",
    "                parts = line.split()\n",
    "                attr_name = parts[1]\n",
    "\n",
    "                if '{' in line:  # nominal attribute\n",
    "                    values = re.search(r\"\\{(.*)\\}\", line).group(1).split(',')\n",
    "                    values = [v.strip() for v in values]\n",
    "                    attributes[attr_name] = {\"type\": \"nominal\", \"values\": values}\n",
    "                elif 'real' in line or 'integer' in line:\n",
    "                    rng_match = re.search(r\"\\[(.*),(.*)\\]\", line)\n",
    "                    if rng_match:\n",
    "                        lo, hi = float(rng_match.group(1)), float(rng_match.group(2))\n",
    "                        attributes[attr_name] = {\"type\": \"numeric\", \"range\": (lo, hi)}\n",
    "                    else:\n",
    "                        attributes[attr_name] = {\"type\": \"numeric\", \"range\": None}\n",
    "\n",
    "            elif line.lower().startswith('@data'):\n",
    "                data_started = True\n",
    "\n",
    "            elif data_started:\n",
    "                row = [v.strip() for v in line.split(',')]\n",
    "                data_rows.append(row)\n",
    "\n",
    "    # Build dataframe\n",
    "    colnames = list(attributes.keys())\n",
    "    df = pd.DataFrame(data_rows, columns=colnames)\n",
    "\n",
    "    # Cast numeric columns properly\n",
    "    for col, meta in attributes.items():\n",
    "        if meta[\"type\"] == \"numeric\":\n",
    "            df[col] = pd.to_numeric(df[col])\n",
    "\n",
    "    return df, attributes\n",
    "\n",
    "\n",
    "def check_violation(instance, attributes):\n",
    "    \"\"\"Check if a pandas Series (row) violates attribute constraints.\"\"\"\n",
    "    violations = {}\n",
    "    for col, meta in attributes.items():\n",
    "        val = instance[col]\n",
    "        if meta[\"type\"] == \"numeric\" and meta[\"range\"]:\n",
    "            lo, hi = meta[\"range\"]\n",
    "            if val < lo or val > hi:\n",
    "                violations[col] = f\"{val} out of range [{lo},{hi}]\"\n",
    "        elif meta[\"type\"] == \"nominal\":\n",
    "            if val not in meta[\"values\"]:\n",
    "                violations[col] = f\"{val} not in {meta['values']}\"\n",
    "    return violations\n",
    "\n",
    "\n",
    "def check_all_violations(df, attributes):\n",
    "    \"\"\"Check all rows in DataFrame for violations and return summary.\"\"\"\n",
    "    violation_summary = {col: 0 for col in attributes.keys()}\n",
    "    detailed = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        vios = check_violation(row, attributes)\n",
    "        if vios:\n",
    "            detailed.append((idx, vios))\n",
    "            for col in vios.keys():\n",
    "                violation_summary[col] += 1\n",
    "\n",
    "    return violation_summary, detailed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "\n",
    "PATH = \"extract_constraints/data/violation_data\"\n",
    "# PATH = \"extract_constraints/data/synthetic_data\"\n",
    "results_path = \"experiment/result/\"\n",
    "os.makedirs(PATH, exist_ok=True)\n",
    "\n",
    "def get_minority_idx(y):\n",
    "    return list(y.unique()).index(y.value_counts().idxmin())\n",
    "\n",
    "classifiers = {\n",
    "        \"LogisticRegression\": LogisticRegression(random_state=42, max_iter=1000),\n",
    "        \"kNN\": KNeighborsClassifier(),\n",
    "        \"DecisionTree\": DecisionTreeClassifier(random_state=42)\n",
    "    }\n",
    "\n",
    "distance_factor=2\n",
    "sparsity_factor=1\n",
    "constraints_factor=3\n",
    "# violation_datasets = ['iris0','pima','wisconsin','paw02a-600-5-70-BI', '04clover5z-800-7-70-BI', '03subcl5-800-7-50-BI']\n",
    "violation_datasets = ['paw02a-600-5-70-BI', '04clover5z-800-7-70-BI', '03subcl5-800-7-50-BI']\n",
    "# violation_datasets = [\"education_dataset\", \"energy_dataset\", \"finance_dataset\", \"fraud_detection_dataset\", \"healthcare_dataset\", \"quality_control_dataset\"]\n",
    "for dataset in violation_datasets:\n",
    "    print(f\"Dataset {dataset}\")\n",
    "    dataset_path = f\"{PATH}/{dataset.replace('.csv','')}\"\n",
    "\n",
    "    overall_start = time.time()\n",
    "    df, attrs = read_keel_dat(f\"{PATH}/{dataset}.dat\")\n",
    "    \n",
    "    X_test = pd.read_csv(f\"{dataset_path}/X_test.csv\")\n",
    "    y_test = pd.read_csv(f\"{dataset_path}/y_test.csv\")\n",
    "    \n",
    "    \n",
    "    # TODO - investigate this\n",
    "    y_test = y_test['Class']\n",
    "    y_test = y_test.astype(str) # 0 to '0'\n",
    "    \n",
    "    # Split X and y\n",
    "    X_full = df.iloc[:, :-1]  # all columns except the last\n",
    "    y_full = df.iloc[:, -1]   # the last column is y\n",
    "    \n",
    "    # Remove test rows from full dataset to get training set\n",
    "    X_train = X_full[~X_full.apply(tuple, axis=1).isin(X_test.apply(tuple, axis=1))]\n",
    "    numerical_cols = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    \n",
    "    y_train = y_full.loc[X_train.index]\n",
    "    X_train = X_train[numerical_cols]\n",
    "    features = numerical_cols\n",
    "    feature_min_max = {feat: (X_train[feat].min(), X_train[feat].max()) for feat in features}\n",
    "    X_train = X_train.to_numpy()\n",
    "    \n",
    "    model = RandomForestClassifier(n_estimators=3, random_state=42, class_weight='balanced')\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    with open(f\"{dataset_path}/constraints.json\", \"r\") as f:\n",
    "        constraints = json.load(f)  # constraints is now a Python dict\n",
    "    \n",
    "    target_class = f\"Class {get_minority_idx(y_train)}\"\n",
    "    print(f\"Target Class: {target_class}\")\n",
    "    \n",
    "    class_counts = pd.Series(y_train).value_counts()\n",
    "    majority_class = class_counts.idxmax()\n",
    "    minority_class = class_counts.idxmin()\n",
    "    majority_count = class_counts.max()\n",
    "    minority_count = class_counts.min()\n",
    "\n",
    "    X_test_minority = X_test[y_test == minority_class]\n",
    "    X_test_minority_np = X_test_minority.to_numpy()\n",
    "\n",
    "    sample_size = min(1, len(X_test_minority_np)) # just in case we want to pass a group of samples later\n",
    "    X_test_np = X_test.to_numpy()\n",
    "\n",
    "    # Sample indices from minority class only\n",
    "    sample_indices = np.random.default_rng().choice(len(X_test_minority_np), size=sample_size, replace=False)\n",
    "    \n",
    "    sample_features = X_test_minority_np[sample_indices]\n",
    "    \n",
    "    # Create corresponding labels\n",
    "    sample_labels = np.full((sample_size, 1), minority_class, dtype=object)\n",
    "\n",
    "    # Combine features and label\n",
    "    sample = np.hstack([sample_features, sample_labels])\n",
    "    \n",
    "    print(sample)\n",
    "    \n",
    "    try:\n",
    "        ga = TwoClassTargetDPG_GA(\n",
    "                        population_size=30,\n",
    "                        mutation_rate=0.3,\n",
    "                        crossover_rate=0.7,\n",
    "                        model=model,\n",
    "                        target_class=minority_class,\n",
    "                        border_class=minority_class,\n",
    "                        n_classes=len(np.unique(y_train)),\n",
    "                        class_bounds=constraints,\n",
    "                        augmentation_mode=\"traditional\",\n",
    "                        boundary_points=None,\n",
    "                        boundary_weight=15.0,\n",
    "                        max_other_prob=0.2,\n",
    "                        diversity_weight=0.5,\n",
    "                        re_inject_threshold=0.5,\n",
    "                        re_inject_ratio=0.2,\n",
    "                        repulsion_weight=4.0,\n",
    "                        feature_order=features,\n",
    "                        random_seed=123,\n",
    "                        default_intervals=feature_min_max,\n",
    "                        sample = sample,\n",
    "                        distance_factor=distance_factor,\n",
    "                        sparsity_factor=sparsity_factor,\n",
    "                        constraints_factor=constraints_factor\n",
    "                        )\n",
    "        \n",
    "        augmentation_percentages = [0.15, 0.3, 0.5]\n",
    "        \n",
    "        for perc in augmentation_percentages:\n",
    "            violations_list = []\n",
    "            print(f\"===========\\n Augmentation Percentage {perc}\\n\")\n",
    "            results_list = []\n",
    "            new_count = int(math.ceil(minority_count * (1 + perc)))\n",
    "            gen_start = time.time()\n",
    "            synthetic_data = ga.generate_samples(num_samples=abs(new_count-minority_count), generations=20, dataset_name=dataset, perc=perc)\n",
    "            \n",
    "            gen_time = time.time() - gen_start\n",
    "            X_train_aug = np.vstack((X_train, synthetic_data))\n",
    "            synthetic_labels = np.full((len(synthetic_data),), minority_class)\n",
    "            y_train_aug = np.concatenate((y_train, synthetic_labels))\n",
    "\n",
    "            \n",
    "            X_train_aug = pd.DataFrame(X_train_aug, columns=features)\n",
    "\n",
    "            df_aug = pd.DataFrame(X_train_aug, columns=features)\n",
    "            df_aug[\"Class\"] = y_train_aug\n",
    "\n",
    "            summary, details = check_all_violations(df_aug, attrs)\n",
    "\n",
    "            # violations = evaluate_constraints(dataset, X_train_aug)            \n",
    "            violations_dict = {\"Dataset\": dataset,\n",
    "                                \"Method\":'DPG-da',\n",
    "                                \"Aug\": perc,\n",
    "                                \"Samples\": new_count,\n",
    "                                \"Number of Violation\": len(details),\n",
    "                                \"Violations\":details,\n",
    "                                \"Rep\":1}\n",
    "            violations_list.append(violations_dict)\n",
    "        \n",
    "            file_exists = os.path.isfile(f\"{results_path}/keel_violations_dpg.csv\")\n",
    "            violations_df = pd.DataFrame(violations_list)\n",
    "            violations_df.to_csv(f\"{results_path}/keel_violations_dpg.csv\", index=False, mode=\"a\", header= not file_exists)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR Dataset {dataset} - {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Performance Section 6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    classification_report\n",
    ")\n",
    "import math\n",
    "\n",
    "from scipy.spatial.distance import euclidean, cityblock, cosine\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "PATH_EVU = \"experiment/data/evolution\"\n",
    "os.makedirs(PATH_EVU, exist_ok=True)\n",
    "\n",
    "class TwoClassTargetDPG_GA:\n",
    "    def __init__(self,\n",
    "                 population_size,\n",
    "                 mutation_rate,\n",
    "                 crossover_rate,\n",
    "                 model,\n",
    "                 target_class,\n",
    "                 border_class,\n",
    "                 n_classes,\n",
    "                 class_bounds,\n",
    "                 augmentation_mode,  # \"traditional\" or \"border\"\n",
    "                 boundary_points,\n",
    "                 boundary_weight,\n",
    "                 max_other_prob,\n",
    "                 diversity_weight,\n",
    "                 re_inject_threshold,  # not used\n",
    "                 re_inject_ratio,        # not used\n",
    "                 repulsion_weight,\n",
    "                 feature_order,\n",
    "                 random_seed,\n",
    "                 default_intervals,\n",
    "                 sample,\n",
    "                 distance_factor,\n",
    "                 sparsity_factor,\n",
    "                 constraints_factor):\n",
    "        random.seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "        self.population_size = population_size\n",
    "        self.mutation_rate = mutation_rate\n",
    "        self.crossover_rate = crossover_rate\n",
    "        self.model = model\n",
    "        self.target_class = target_class % n_classes\n",
    "        self.border_class = border_class % n_classes\n",
    "        self.n_classes = n_classes\n",
    "        self.class_bounds = class_bounds\n",
    "        self.augmentation_mode = augmentation_mode\n",
    "        self.boundary_points = boundary_points  # not used in \"traditional\" mode\n",
    "        self.boundary_weight = boundary_weight\n",
    "        self.max_other_prob = max_other_prob\n",
    "        self.diversity_weight = diversity_weight\n",
    "        self.re_inject_threshold = re_inject_threshold\n",
    "        self.re_inject_ratio = re_inject_ratio\n",
    "        self.repulsion_weight = repulsion_weight\n",
    "        self.feature_order = feature_order\n",
    "        # Use provided bounds if available; otherwise fallback to default_intervals.\n",
    "        self.target_intervals = self.class_bounds.get(f\"Class {self.target_class}\", default_intervals)\n",
    "        self.sample = sample\n",
    "        self.distance_factor = distance_factor\n",
    "        self.sparsity_factor = sparsity_factor\n",
    "        self.constraints_factor = constraints_factor\n",
    "        \n",
    "    def random_individual(self):\n",
    "        original = self.sample.flatten()[:-1]  # Assuming last value is the target\n",
    "        num_features = len(original)\n",
    "        min_changes = int(0.05 * num_features)\n",
    "        max_changes = int(0.1 * num_features)\n",
    "        num_changes = random.randint(min_changes, max_changes)\n",
    "        \n",
    "        # Choose random feature indices to change\n",
    "        change_indices = random.sample(range(num_features), num_changes)\n",
    "        \n",
    "        # Copy original\n",
    "        new_individual = original.copy()\n",
    "        \n",
    "        # Apply random changes to selected features\n",
    "        for i in change_indices:\n",
    "            feat_name = self.feature_order[i]\n",
    "            low, high = self.target_intervals[feat_name]\n",
    "            new_individual[i] = random.uniform(low, high)\n",
    "    \n",
    "        return new_individual if self.augmentation_mode == \"traditional\" else new_individual.tolist()\n",
    "\n",
    "    def initialize_population(self):\n",
    "        if self.augmentation_mode == \"traditional\":\n",
    "            return np.array([self.random_individual() for _ in range(self.population_size)])\n",
    "        else:\n",
    "            return [self.random_individual() for _ in range(self.population_size)]\n",
    "\n",
    "    def mutate(self, pop: np.ndarray) -> np.ndarray:\n",
    "        for i in range(len(pop)):\n",
    "            if random.random() < self.mutation_rate:\n",
    "                for j, feat in enumerate(self.feature_order):\n",
    "                    low, high = self.target_intervals[feat]\n",
    "                    r = (high - low) * 0.05\n",
    "                    delta = random.uniform(-r, r)\n",
    "                    pop[i, j] += delta\n",
    "                    pop[i, j] = max(low, min(pop[i, j], high))\n",
    "        return pop\n",
    "    # check size\n",
    "    def mutate_individual(self, ind: List[float]) -> List[float]:\n",
    "        new_ind = ind.copy()\n",
    "        for j, feat in enumerate(self.feature_order):\n",
    "            low, high = self.target_intervals[feat]\n",
    "            r = (high - low) * 0.05\n",
    "            delta = random.uniform(-r, r)\n",
    "            new_ind[j] += delta\n",
    "            new_ind[j] = max(low, min(new_ind[j], high))\n",
    "        return new_ind\n",
    "\n",
    "    def crossover(self, pop: np.ndarray) -> np.ndarray:\n",
    "        children = []\n",
    "        for i in range(0, len(pop), 2):\n",
    "            p1, p2 = pop[i], pop[i + 1]\n",
    "            if random.random() < self.crossover_rate:\n",
    "                point = random.randint(1, len(p1) - 1)\n",
    "                c1 = np.concatenate([p1[:point], p2[point:]])\n",
    "                c2 = np.concatenate([p2[:point], p1[point:]])\n",
    "                children.extend([c1, c2])\n",
    "            else:\n",
    "                children.extend([p1, p2])\n",
    "        return np.array(children)\n",
    "\n",
    "    def crossover_individuals(self, ind1: List[float], ind2: List[float]) -> Tuple[List[float], List[float]]:\n",
    "        point = random.randint(1, len(ind1) - 1)\n",
    "        c1 = ind1[:point] + ind2[point:]\n",
    "        c2 = ind2[:point] + ind1[point:]\n",
    "        return c1, c2\n",
    "\n",
    "    def fitness_function_traditional(self, pop: np.ndarray) -> np.ndarray:\n",
    "        fitness = np.zeros(len(pop))        \n",
    "        sample_features = self.sample.flatten()[:-1]  # remove target\n",
    "        \n",
    "        for i, individual in enumerate(pop):\n",
    "            X_pred = individual.reshape(1, -1)\n",
    "            predicted_class = self.model.predict(X_pred)\n",
    "            constraints_score = round(self.calculate_constraint_violation(individual),4)\n",
    "            distance_score = round(self.calculate_distance(sample_features, individual), 4)\n",
    "            sparsity_score = round(self.calculate_sparsity(sample_features, individual), 4)\n",
    "            \n",
    "            fitness[i] += (1 - constraints_score) * self.constraints_factor\n",
    "            fitness[i] += distance_score * self.distance_factor\n",
    "            fitness[i] += (1 - sparsity_score) * self.sparsity_factor\n",
    "            \n",
    "            if predicted_class != self.target_class:\n",
    "                fitness[i] *= -1\n",
    "        return fitness\n",
    "\n",
    "    def base_border_fitness(self, ind: List[float]) -> float:\n",
    "        INVALID_FITNESS = -50.0\n",
    "        for feat in self.feature_order:\n",
    "            low, high = self.target_intervals[feat]\n",
    "            val = ind[self.feature_order.index(feat)]\n",
    "            if not (low <= val <= high):\n",
    "                return INVALID_FITNESS\n",
    "        arr2d = np.array(ind)\n",
    "        probs = self.model.predict_proba([arr2d])[0]\n",
    "        pred = self.model.predict([arr2d])[0]\n",
    "        if pred != self.target_class:\n",
    "            return INVALID_FITNESS\n",
    "        p_t = probs[self.target_class]\n",
    "        p_b = probs[self.border_class]\n",
    "        boundary_score = 1.0 - abs(p_t - p_b)\n",
    "        for c in range(self.n_classes):\n",
    "            if c not in (self.target_class, self.border_class):\n",
    "                if probs[c] > self.max_other_prob:\n",
    "                    return INVALID_FITNESS\n",
    "        return 5.0 * p_t + 3.0 * boundary_score\n",
    "\n",
    "    def individual_diversity(self, ind: List[float], pop: List[List[float]]) -> float:\n",
    "        distances = [np.linalg.norm(np.array(ind) - np.array(other)) for other in pop if other != ind]\n",
    "        return np.mean(distances) if distances else 0.0\n",
    "\n",
    "    def min_distance_to_others(self, ind: List[float], pop: List[List[float]]) -> float:\n",
    "        distances = [np.linalg.norm(np.array(ind) - np.array(other)) for other in pop if other != ind]\n",
    "        return min(distances) if distances else 0.0\n",
    "\n",
    "    def distance_to_boundary_line(self, ind: List[float]) -> float:\n",
    "        return 0.05\n",
    "\n",
    "    def compute_population_diversity(self, pop: List[List[float]]) -> float:\n",
    "        if len(pop) < 2:\n",
    "            return 0.0\n",
    "        distances = [np.linalg.norm(np.array(ind1) - np.array(ind2))\n",
    "                     for i, ind1 in enumerate(pop) for ind2 in pop[i+1:]]\n",
    "        return np.mean(distances)\n",
    "\n",
    "    def total_fitness(self, ind: List[float], pop: List[List[float]]) -> float:\n",
    "        base = self.base_border_fitness(ind)\n",
    "        if base < -40:\n",
    "            return base\n",
    "        div = self.individual_diversity(ind, pop)\n",
    "        div_bonus = self.diversity_weight * div\n",
    "        min_d = self.min_distance_to_others(ind, pop)\n",
    "        rep_bonus = self.repulsion_weight * min_d\n",
    "        dist_line = self.distance_to_boundary_line(ind)\n",
    "        line_bonus = 1.0 / (1.0 + dist_line) * self.boundary_weight\n",
    "        penalty = -50.0 if dist_line > 0.1 else 0.0\n",
    "        return base + div_bonus + rep_bonus + line_bonus + penalty\n",
    "\n",
    "    def get_fitness(self, ind: List[float], pop: List[List[float]]) -> float:\n",
    "        if self.augmentation_mode == \"traditional\":\n",
    "            arr = np.array(ind).reshape(1, -1)\n",
    "            return self.fitness_function_traditional(arr)[0]\n",
    "        else:\n",
    "            return self.total_fitness(ind, pop)\n",
    "\n",
    "    def select_parents_tournament(self, pop: List[List[float]], t_size=3):\n",
    "        def tournament():\n",
    "            candidates = random.sample(pop, t_size)\n",
    "            return max(candidates, key=lambda c: self.get_fitness(c, pop))\n",
    "        return tournament(), tournament()\n",
    "        \n",
    "    def calculate_sparsity(self, original_sample: np.ndarray, individual_sample: np.ndarray) -> float:\n",
    "        # Ensure inputs are 1D arrays\n",
    "        original_sample = original_sample.flatten()\n",
    "        individual_sample = individual_sample.flatten()\n",
    "        \n",
    "        # Count how many features differ\n",
    "        changed_features = np.sum(original_sample != individual_sample)\n",
    "        \n",
    "        # Ratio of changed features\n",
    "        # 1 -> every feature changed\n",
    "        # 0 -> none\n",
    "        sparsity = changed_features / len(original_sample)\n",
    "        return sparsity\n",
    "    \n",
    "    def calculate_distance(self, original_sample: np.ndarray, individual_sample: np.ndarray) -> float:\n",
    "        original_sample = np.asarray(original_sample)\n",
    "        individual_sample = np.asarray(individual_sample)\n",
    "\n",
    "        # Avoid division by zero\n",
    "        denominator = np.where(original_sample == 0, 1e-8, original_sample)\n",
    "        \n",
    "        relative_distance = np.abs((individual_sample - original_sample) / denominator)\n",
    "        return np.mean(relative_distance)\n",
    "    \n",
    "    def calculate_constraint_violation(self, individual: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the percentage of features that are outside the allowed target_intervals.\n",
    "        \n",
    "        Parameters:\n",
    "            individual (np.ndarray): 1D array of feature values for a candidate individual.\n",
    "        \n",
    "        Returns:\n",
    "            float: Ratio (0 to 1) of features that violate their interval constraints.\n",
    "        \"\"\"\n",
    "        violations = 0\n",
    "        total_features = len(self.feature_order)\n",
    "\n",
    "        for i, feat in enumerate(self.feature_order):\n",
    "            low, high = self.target_intervals[feat]\n",
    "            if not (low <= individual[i] <= high):\n",
    "                violations += 1\n",
    "\n",
    "        return violations / total_features\n",
    "\n",
    "    # class -00 1\n",
    "    # features 0 a 1\n",
    "    def evolve(self, generations=20, tournament_size=3, stagnation_limit=5, dataset_name=None, perc=None):\n",
    "        if self.augmentation_mode == \"traditional\":\n",
    "            pop = self.initialize_population()\n",
    "\n",
    "            best_fit_so_far = float('-inf')\n",
    "            no_improve = 0\n",
    "            log_df = pd.DataFrame(columns=[\"generation\", \"fitness\", \"individual\"])\n",
    "            for gen in range(generations):\n",
    "                # print(f\"Generation: {gen}\")\n",
    "                fitness = self.fitness_function_traditional(pop)\n",
    "                best_ind = pop[np.argmax(fitness)]\n",
    "                best_val = np.max(fitness)\n",
    "\n",
    "                log_df = pd.concat([\n",
    "                    log_df,\n",
    "                    pd.DataFrame([{\n",
    "                        \"generation\": gen,\n",
    "                        \"fitness\": float(best_val),\n",
    "                        \"individual\": best_ind.tolist(),\n",
    "                    }])\n",
    "                ], ignore_index=True)\n",
    "\n",
    "                # print(f\"Best Individual {best_ind}\")\n",
    "                new_pop = [best_ind]\n",
    "\n",
    "                while len(new_pop) < self.population_size:\n",
    "                    # print(f\"NEW POP {new_pop}\")\n",
    "                    p1, p2 = self.select_parents_tournament(list(pop), tournament_size)\n",
    "                    # print(\"Tournament\")\n",
    "                    if random.random() < self.crossover_rate:\n",
    "                        # print(\"Crossover\")\n",
    "                        # print(f\"TYPE: {type(p1)}\")\n",
    "                        c1, c2 = self.crossover_individuals(list(p1), list(p2))\n",
    "                        # print(\"Crossover!\")\n",
    "                    else:\n",
    "                        c1, c2 = p1[:], p2[:]\n",
    "                    if random.random() < self.mutation_rate:\n",
    "                        # print(f\"C1: {len(c1)}\")\n",
    "                        c1 = self.mutate_individual(c1)\n",
    "                        # print(\"Mutation\")\n",
    "                    if random.random() < self.mutation_rate:\n",
    "                        # print(f\"C2: {len(c2)}\")\n",
    "                        c2 = self.mutate_individual(c2)\n",
    "                        # print(\"Mutation2\")\n",
    "                    new_pop.extend([c1, c2])\n",
    "                pop = new_pop[:self.population_size]\n",
    "                pop = np.array(pop)\n",
    "                if best_val > best_fit_so_far:\n",
    "                    best_fit_so_far = best_val\n",
    "                    no_improve = 0\n",
    "                else:\n",
    "                    no_improve += 1\n",
    "                if no_improve >= stagnation_limit:\n",
    "                    print(f\"[Early Stop Gen {gen}] No improvement for {stagnation_limit} generations.\")\n",
    "                    break\n",
    "            # Save the log to CSV or Parquet\n",
    "            log_df.to_csv(f\"{PATH_EVU}/{dataset_name}_{perc}_evolution_log.csv\", index=False)\n",
    "            return pop\n",
    "        else:\n",
    "            pop = self.initialize_population()\n",
    "            best_fit_so_far = float('-inf')\n",
    "            no_improve = 0\n",
    "            for gen in range(generations):\n",
    "                best_ind = max(pop, key=lambda c: self.get_fitness(c, pop))\n",
    "                best_val = self.get_fitness(best_ind, pop)\n",
    "                new_pop = [best_ind]\n",
    "                while len(new_pop) < self.population_size:\n",
    "                    p1, p2 = self.select_parents_tournament(pop, tournament_size)\n",
    "                    if random.random() < self.crossover_rate:\n",
    "                        c1, c2 = self.crossover_individuals(p1, p2)\n",
    "                    else:\n",
    "                        c1, c2 = p1[:], p2[:]\n",
    "                    if random.random() < self.mutation_rate:\n",
    "                        c1 = self.mutate_individual(c1)\n",
    "                    if random.random() < self.mutation_rate:\n",
    "                        c2 = self.mutate_individual(c2)\n",
    "                    new_pop.extend([c1, c2])\n",
    "                pop = new_pop[:self.population_size]\n",
    "                if best_val > best_fit_so_far:\n",
    "                    best_fit_so_far = best_val\n",
    "                    no_improve = 0\n",
    "                else:\n",
    "                    no_improve += 1\n",
    "                if no_improve >= stagnation_limit:\n",
    "                    print(f\"[Early Stop Gen {gen}] No improvement for {stagnation_limit} generations.\")\n",
    "                    break\n",
    "            return pop\n",
    "    \n",
    "    def generate_samples(self, num_samples=5, generations=20, real_minority_samples=[], dataset_name=None, perc=None):\n",
    "        print(\"Generate samples\")\n",
    "        print(\"Classe\", self.target_class)\n",
    "        real_minority_samples = list(real_minority_samples)\n",
    "        results = []\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            self.sample = random.choice(real_minority_samples)\n",
    "            print(f\"Minority Starting point: {self.sample}\")\n",
    "            \n",
    "            pop = self.evolve(generations=generations, dataset_name=dataset_name, perc=perc)\n",
    "            fitness_scores = self.fitness_function_traditional(pop)\n",
    "            best_ind = pop[np.argmax(fitness_scores)]\n",
    "            results.append(best_ind)\n",
    "            best_ind_with_label = np.append(best_ind, self.target_class)\n",
    "            real_minority_samples.append(best_ind_with_label)\n",
    "\n",
    "        return np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PATH = \"experiment/data/constraints\"\n",
    "results_path = \"experiment/result/dpg_aug_result_macro.csv\"\n",
    "\n",
    "os.listdir(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "\n",
    "PATH = \"experiment/data/constraints\"\n",
    "results_path = \"experiment/result/dpg_aug_result.csv\"\n",
    "\n",
    "os.makedirs(PATH, exist_ok=True)\n",
    "\n",
    "distance_factor=2\n",
    "sparsity_factor=1\n",
    "constraints_factor=3\n",
    "\n",
    "def get_minority_idx(y):\n",
    "    return list(y.unique()).index(y.value_counts().idxmin())\n",
    "\n",
    "classifiers = {\n",
    "        \"LogisticRegression\": LogisticRegression(random_state=42, max_iter=1000),\n",
    "        \"kNN\": KNeighborsClassifier(),\n",
    "        \"DecisionTree\": DecisionTreeClassifier(random_state=42)\n",
    "    }\n",
    "\n",
    "exclusion_list = [\"education_dataset\", \"energy_dataset\", \"finance_dataset\", \"fraud_detection_dataset\", \"healthcare_dataset\", \"quality_control_dataset\"]\n",
    "\n",
    "# for dataset in ['ecoli']:\n",
    "for dataset in os.listdir(PATH):\n",
    "    if dataset in exclusion_list:\n",
    "        continue  # Skip excluded datasets\n",
    "    print(f\"Dataset {dataset}\")\n",
    "    overall_start = time.time()\n",
    "    \n",
    "    dataset_path = f\"{PATH}/{dataset}\"\n",
    "    \n",
    "    X_test = pd.read_csv(f\"{dataset_path}/X_test.csv\")\n",
    "    y_test = pd.read_csv(f\"{dataset_path}/y_test.csv\")\n",
    "    \n",
    "    # TODO - investigate this\n",
    "    if 'target' in y_test.columns:\n",
    "        y_test = y_test['target']\n",
    "\n",
    "    full_data = pd.read_csv(f\"{dataset_path}/{dataset}.csv\")  # Replace with actual file name\n",
    "\n",
    "    # Split X and y\n",
    "    X_full = full_data.iloc[:, :-1]  # all columns except the last\n",
    "    y_full = full_data.iloc[:, -1]   # the last column is y\n",
    "\n",
    "    # Remove test rows from full dataset to get training set\n",
    "    X_train = X_full[~X_full.apply(tuple, axis=1).isin(X_test.apply(tuple, axis=1))]\n",
    "    numerical_cols = X_train.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    y_train = y_full.loc[X_train.index]\n",
    "    \n",
    "    X_train = X_train[numerical_cols]\n",
    "    features = numerical_cols\n",
    "    feature_min_max = {feat: (X_train[feat].min(), X_train[feat].max()) for feat in features}\n",
    "    X_train = X_train.to_numpy()\n",
    "    \n",
    "    model = RandomForestClassifier(n_estimators=3, random_state=42, class_weight='balanced')\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # y_pred = model.predict(X_test)\n",
    "\n",
    "    # Generate classification report\n",
    "    # report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    with open(f\"{dataset_path}/constraints.json\", \"r\") as f:\n",
    "        constraints = json.load(f)  # constraints is now a Python dict\n",
    "    \n",
    "    target_class = f\"Class {get_minority_idx(y_train)}\"\n",
    "    print(f\"Target Class: {target_class}\")\n",
    "    \n",
    "    # these counts are on the training\n",
    "    class_counts = pd.Series(y_train).value_counts()\n",
    "    majority_class = class_counts.idxmax()\n",
    "    minority_class = class_counts.idxmin()\n",
    "    majority_count = class_counts.max()\n",
    "    minority_count = class_counts.min()\n",
    "\n",
    "    X_test_minority = X_test[y_test == minority_class]\n",
    "    X_test_minority_np = X_test_minority.to_numpy()\n",
    "\n",
    "    # sample_size = min(1, len(X_test_minority_np)) # just in case we want to pass a group of samples later\n",
    "    sample_size = len(X_test_minority_np) # just in case we want to pass a group of samples later\n",
    "    X_test_np = X_test.to_numpy()\n",
    "\n",
    "    # Sample indices from minority class only\n",
    "    sample_indices = np.random.default_rng().choice(len(X_test_minority_np), size=sample_size, replace=False)\n",
    "    \n",
    "    sample_features = X_test_minority_np[sample_indices]\n",
    "\n",
    "    # Create corresponding labels\n",
    "    sample_labels = np.full((sample_size, 1), minority_class)\n",
    "\n",
    "    # Combine features and label\n",
    "    sample = np.hstack([sample_features, sample_labels])\n",
    "    \n",
    "    # print(sample)\n",
    "    print(sample)\n",
    "    print(len(sample))\n",
    "    print(minority_count)\n",
    "    # break\n",
    "    \n",
    "    try:\n",
    "        ga = TwoClassTargetDPG_GA(\n",
    "                        population_size=30,\n",
    "                        mutation_rate=0.3,\n",
    "                        crossover_rate=0.7,\n",
    "                        model=model,\n",
    "                        target_class=minority_class,\n",
    "                        border_class=minority_class,\n",
    "                        n_classes=len(np.unique(y_train)),\n",
    "                        class_bounds=constraints,\n",
    "                        augmentation_mode=\"traditional\",\n",
    "                        boundary_points=None,\n",
    "                        boundary_weight=15.0,\n",
    "                        max_other_prob=0.2,\n",
    "                        diversity_weight=0.5,\n",
    "                        re_inject_threshold=0.5,\n",
    "                        re_inject_ratio=0.2,\n",
    "                        repulsion_weight=4.0,\n",
    "                        feature_order=features,\n",
    "                        random_seed=123,\n",
    "                        default_intervals=feature_min_max,\n",
    "                        sample = sample,\n",
    "                        distance_factor=distance_factor,\n",
    "                        sparsity_factor=sparsity_factor,\n",
    "                        constraints_factor=constraints_factor\n",
    "                        )\n",
    "        \n",
    "        augmentation_percentages = [0.15, 0.3, 0.5]\n",
    "        \n",
    "        for perc in augmentation_percentages:\n",
    "            print(f\"===========\\n Augmentation Percentage {perc}\\n\")\n",
    "            results_list = []\n",
    "            desired_minority_count = (perc * majority_count) / (1 - perc)\n",
    "            additional_samples_needed = int(desired_minority_count - minority_count)\n",
    "\n",
    "            print(f\"Augmentation {perc*100:.0f}%:\")\n",
    "            print(f\"Current minority count: {minority_count}\")\n",
    "            print(f\"Target minority count: {int(desired_minority_count)}\")\n",
    "            print(f\"Additional samples needed: {max(0, additional_samples_needed)}\\n\")\n",
    "            \n",
    "            new_count = max(0, additional_samples_needed)\n",
    "            if new_count > 0:\n",
    "                gen_start = time.time()\n",
    "\n",
    "                # synthetic_data = ga.generate_samples(num_samples=new_count, generations=20, dataset_name=dataset, perc=perc)\n",
    "                # gen_time = time.time() - gen_start\n",
    "                # X_train_aug = np.vstack((X_train, synthetic_data))\n",
    "                \n",
    "                # -----\n",
    "                synthetic_data = ga.generate_samples(num_samples=new_count, generations=20, real_minority_samples=sample, dataset_name=dataset, perc=perc)\n",
    "                gen_time = time.time() - gen_start\n",
    "\n",
    "                # print(\"synthetic data\")\n",
    "                # synthetic_df = pd.DataFrame(synthetic_data, columns=features)\n",
    "                # synthetic_save_path = f\"{dataset_path}/synthetic_{int(perc*100)}.csv\"\n",
    "                # synthetic_df.to_csv(synthetic_save_path, index=False)\n",
    "                \n",
    "                X_train_aug = np.vstack((X_train, synthetic_data))\n",
    "                # -----\n",
    "\n",
    "                synthetic_labels = np.full((len(synthetic_data),), minority_class)\n",
    "                print(f\"Original data: {X_train}\")\n",
    "                print(f\"Synthetically added: {synthetic_data}\")\n",
    "                \n",
    "                # Merge y_train and synthetic_labels\n",
    "                y_train_aug = np.concatenate((y_train, synthetic_labels))\n",
    "                \n",
    "                final_minority_count = np.sum(y_train_aug == minority_class)\n",
    "                final_majority_count = np.sum(y_train_aug == majority_class)\n",
    "                synthetic_added = final_minority_count - minority_count\n",
    "                for clf_name, clf in classifiers.items():\n",
    "                    # Augmented data Stat\n",
    "                    clf.fit(X_train_aug, y_train_aug)\n",
    "                    y_pred_aug = clf.predict(X_test)\n",
    "                    f1_aug = f1_score(y_test, y_pred_aug, average='macro')\n",
    "                    acc_aug = accuracy_score(y_test, y_pred_aug)\n",
    "                    precision_aug = precision_score(y_test, y_pred_aug, average='macro', zero_division=0)\n",
    "                    recall_aug = recall_score(y_test, y_pred_aug, average='macro', zero_division=0)\n",
    "\n",
    "                    results_list.append({\n",
    "                        \"Method\": \"DPG-da\",\n",
    "                        \"Augmentation_Percentage\": perc,\n",
    "                        \"Augmentation_Level\": f\"{perc*100:.2f}%\",\n",
    "                        \"Repetition\": 1,\n",
    "                        \"Classifier\": clf_name,\n",
    "                        \"F1_aug\": f1_aug,\n",
    "                        \"Accuracy_aug\": acc_aug,\n",
    "                        \"Precision_aug\": precision_aug,\n",
    "                        \"Recall_aug\": recall_aug,            \n",
    "                        \"Original_Minority_Count\": minority_count,\n",
    "                        \"Synthetic_Samples_Added\": synthetic_added,\n",
    "                        \"Final_Minority_Count\": final_minority_count,\n",
    "                        \"Majority_Count\": final_majority_count,\n",
    "                        \"Run_Time\": gen_time   # Time only for synthetic sample generation.\n",
    "                    })\n",
    "                results_df = pd.DataFrame(results_list)\n",
    "\n",
    "                aggregated_df = results_df.groupby([\"Method\", \"Augmentation_Percentage\", \"Classifier\"]).agg(\n",
    "                            Avg_F1_aug=(\"F1_aug\", \"mean\"),\n",
    "                            Std_F1_aug=(\"F1_aug\", \"std\"),\n",
    "                            Avg_Accuracy_aug=(\"Accuracy_aug\", \"mean\"),\n",
    "                            Std_Accuracy_aug=(\"Accuracy_aug\", \"std\"),\n",
    "                            Avg_Precision_aug=(\"Precision_aug\", \"mean\"),\n",
    "                            Std_Precision_aug=(\"Precision_aug\", \"std\"),\n",
    "                            Avg_Recall_aug=(\"Recall_aug\", \"mean\"),\n",
    "                            Std_Recall_aug=(\"Recall_aug\", \"std\"),\n",
    "                            Avg_Run_Time=(\"Run_Time\", \"mean\"),\n",
    "                            Std_Run_Time=(\"Run_Time\", \"std\"),\n",
    "                            Augmentation_Level=(\"Augmentation_Level\", \"first\"),\n",
    "                            Original_Minority_Count=(\"Original_Minority_Count\", \"first\"),\n",
    "                            Synthetic_Samples_Added=(\"Synthetic_Samples_Added\", \"first\"),\n",
    "                            Final_Minority_Count=(\"Final_Minority_Count\", \"first\"),\n",
    "                            Majority_Count=(\"Majority_Count\", \"first\")\n",
    "                    ).reset_index()\n",
    "\n",
    "                overall_end = time.time()\n",
    "                aggregated_df['Dataset'] = dataset.replace(\".csv\",\"\")\n",
    "                aggregated_df['Time'] = round(overall_end - overall_start,2)\n",
    "                # Save aggregated results to CSV.\n",
    "                file_exists = os.path.isfile(results_path)\n",
    "                aggregated_df.to_csv(results_path, index=False, mode='a', header=not file_exists)\n",
    "                print(f\"\\nTotal execution time: {overall_end - overall_start:.2f} seconds\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR Dataset: {dataset} - {e}\")\n",
    "        break\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecai_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
